# Install requirements
pip install -r requirements.txt

# Download models from HuggingFace
# Q8 model (8.0GB):
# Visit: https://huggingface.co/Florents-Tselai/Meltemi-llamafile/tree/main
# Download: Meltemi-7B-Instruct-v1.5-Q8_0.gguf

# Compressed models:
# Visit: https://huggingface.co/mradermacher/Meltemi-7B-Instruct-v1.5-i1-GGUF/tree/main
# Download options:
# - Q4 Compressed:  Meltemi-7B-Instruct-v1.5.i1-IQ4_XS.gguf (4.07GB)
# - Q2 Compressed:  Meltemi-7B-Instruct-v1.5.i1-IQ2_M.gguf (2.64GB)

# Start Ollama server
ollama serve

# Create standard models
ollama create mistral -f Modelfile
ollama create meltemi_q8 -f Modelfile.meltemi_q8

# Create compressed models
ollama create mistral_compressed -f Modelfile.mistral_compressed
ollama create meltemi_compressed_q4 -f Modelfile.meltemi_compressed_q4
ollama create meltemi_compressed_q2 -f Modelfile.meltemi_compressed_q2

# Check installed models
ollama list

# To switch between models, edit config.py and set SELECTED_MODEL to one of:
# - ModelType.MISTRAL              # Standard Mistral (4.1GB)
# - ModelType.MELTEMI_Q8          # Full Meltemi Q8 (8.0GB)
# - ModelType.MISTRAL_COMPRESSED  # Compressed Mistral Q4_K (4.07GB)
# - ModelType.MELTEMI_COMPRESSED_Q4 # Compressed Meltemi Q4_XS (4.07GB)
# - ModelType.MELTEMI_COMPRESSED_Q2 # Ultra Compressed Meltemi IQ2_M (2.64GB)

# Run the test script to verify model
python test_indexer.py

# Run the main application
python main.py